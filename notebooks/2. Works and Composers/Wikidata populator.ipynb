{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#database connection settings\n",
    "import psycopg2\n",
    "\n",
    "db_name = \"traviato_development\"\n",
    "db_host = \"localhost\"\n",
    "db_port = \"5432\"\n",
    "db_user = \"lievgarcia\"\n",
    "db_pwd = \"traviato81\"\n",
    "\n",
    "conn = psycopg2.connect(database=db_name, user=db_user, password=db_pwd, host=db_host, port=db_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### COMPOSERS AKA UPDATE\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0' }\n",
    "\n",
    "cursor = conn.cursor()  \n",
    "cursor.execute(\"SELECT distinct name, url, uri FROM composers c WHERE aka IS NULL\")\n",
    "comp_df = pd.DataFrame(cursor.fetchall(), columns=['name', 'url', 'uri'])\n",
    "\n",
    "for idx,row in comp_df.iterrows():\n",
    "    \n",
    "    wikitext = ''\n",
    "    aliases = []    \n",
    "    scrape = True\n",
    "    \n",
    "    url = row['url']\n",
    "    r  = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"html5lib\")    \n",
    "    try:\n",
    "        wikidata = json.loads(str(soup.body.text))\n",
    "        uri = list(wikidata['entities'].keys())[0]\n",
    "    except:\n",
    "        scrape = None\n",
    "    \n",
    "    #getting all aliases, also known as for item\n",
    "    try:\n",
    "        if scrape:\n",
    "            for k in wikidata['entities'][uri]['aliases']:\n",
    "    #             print(wikidata['entities'][uri]['aliases'][k][0]['value'])\n",
    "                aliases.append(wikidata['entities'][uri]['aliases'][k][0]['value'])\n",
    "            aliases = '; '.join(aliases)\n",
    "            update = \"UPDATE COMPOSERS SET aka = '\" + aliases.replace(\"'\", \"''\") + \"' WHERE uri = '\" + uri + \"'; COMMIT;\"            \n",
    "            cursor.execute(update)\n",
    "#             print(update)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPOSERS WIKIPEDIA TEXT UPDATE\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0' }\n",
    "        \n",
    "cursor = conn.cursor()  \n",
    "cursor.execute(\"SELECT distinct name, url, uri FROM composers c WHERE wikipedia_text IS NULL\")\n",
    "comp_df = pd.DataFrame(cursor.fetchall(), columns=['name', 'url', 'uri'])\n",
    "\n",
    "for idx,row in comp_df.iterrows():\n",
    "    \n",
    "    wikitext = ''\n",
    "    aliases = []    \n",
    "    scrape = True\n",
    "    \n",
    "    url = row['url']\n",
    "    r  = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"html5lib\")    \n",
    "    try:\n",
    "        wikidata = json.loads(str(soup.body.text))\n",
    "        uri = list(wikidata['entities'].keys())[0]\n",
    "    except:\n",
    "        scrape = None\n",
    "    \n",
    "    #getting all wiki text on their page, also known as for item        \n",
    "    try:\n",
    "        if scrape:\n",
    "            wiki_url = wikidata['entities'][uri]['sitelinks']['ruwiki']['url']\n",
    "            r_wiki  = requests.get(wiki_url, headers=headers)\n",
    "            soup = BeautifulSoup(r_wiki.text, \"html5lib\")    \n",
    "            wikitext = soup.body.get_text()      \n",
    "            update = \"UPDATE COMPOSERS SET wikipedia_text = '\" + wikitext.replace(\"'\", \"''\") + \"' WHERE uri = '\" + uri + \"'; COMMIT;\"            \n",
    "            cursor.execute(update)            \n",
    "    except: \n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pageid', 'ns', 'title', 'lastrevid', 'modified', 'type', 'id', 'labels', 'descriptions', 'aliases', 'claims', 'sitelinks'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#database connection settings\n",
    "import psycopg2\n",
    "\n",
    "db_name = \"traviato_development\"\n",
    "db_host = \"localhost\"\n",
    "db_port = \"5432\"\n",
    "db_user = \"lievgarcia\"\n",
    "db_pwd = \"traviato81\"\n",
    "\n",
    "conn = psycopg2.connect(database=db_name, user=db_user, password=db_pwd, host=db_host, port=db_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKS AKA UPDATE\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0' }\n",
    "\n",
    "cursor = conn.cursor()  \n",
    "cursor.execute(\"SELECT distinct name, url, uri FROM works c WHERE aka IS NULL\")\n",
    "work_df = pd.DataFrame(cursor.fetchall(), columns=['name', 'url', 'uri'])\n",
    "\n",
    "for idx,row in work_df.iterrows():\n",
    "    \n",
    "    wikitext = ''\n",
    "    aliases = []    \n",
    "    scrape = True\n",
    "    \n",
    "    url = row['url']\n",
    "    r  = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"html5lib\")    \n",
    "    try:\n",
    "        wikidata = json.loads(str(soup.body.text))\n",
    "        uri = list(wikidata['entities'].keys())[0]\n",
    "    except:\n",
    "        scrape = None\n",
    "    \n",
    "    #getting all aliases, also known as for item\n",
    "    try:\n",
    "        if scrape:\n",
    "            for k in wikidata['entities'][uri]['aliases']:\n",
    "    #             print(wikidata['entities'][uri]['aliases'][k][0]['value'])\n",
    "                aliases.append(wikidata['entities'][uri]['aliases'][k][0]['value'])\n",
    "            aliases = '; '.join(aliases)\n",
    "            update = \"UPDATE WORKS SET aka = '\" + aliases.replace(\"'\", \"''\") + \"' WHERE uri = '\" + uri + \"'; COMMIT;\"            \n",
    "            cursor.execute(update)\n",
    "#             print(update)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKS WIKIPEDIA TEXT UPDATE\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0' }\n",
    "        \n",
    "cursor = conn.cursor()  \n",
    "cursor.execute(\"SELECT distinct name, url, uri FROM works c WHERE wikipedia_text IS NULL\")\n",
    "work_df = pd.DataFrame(cursor.fetchall(), columns=['name', 'url', 'uri'])\n",
    "\n",
    "for idx,row in work_df.iterrows():\n",
    "    \n",
    "    wikitext = ''\n",
    "    aliases = []    \n",
    "    scrape = True\n",
    "    \n",
    "    url = row['url']\n",
    "    r  = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"html5lib\")    \n",
    "    try:\n",
    "        wikidata = json.loads(str(soup.body.text))\n",
    "        uri = list(wikidata['entities'].keys())[0]\n",
    "    except:\n",
    "        scrape = None\n",
    "    \n",
    "    #getting all wiki text on their page, also known as for item        \n",
    "    try:\n",
    "        if scrape:\n",
    "            wiki_url = wikidata['entities'][uri]['sitelinks']['enwiki']['url']\n",
    "            r_wiki  = requests.get(wiki_url, headers=headers)\n",
    "            soup = BeautifulSoup(r_wiki.text, \"html5lib\")    \n",
    "            wikitext = soup.body.get_text()      \n",
    "            update = \"UPDATE WORKS SET wikipedia_text = '\" + wikitext.replace(\"'\", \"''\") + \"' WHERE uri = '\" + uri + \"'; COMMIT;\"            \n",
    "            cursor.execute(update)            \n",
    "    except: \n",
    "        print(wikidata['entities'][uri]['sitelinks'].keys())        \n",
    "#         None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
